# ğŸ½ï¸ Glovo Dish Scraper API

A comprehensive REST API for scraping restaurant data from Glovo platform, featuring dish recommendations, establishment discovery, and interactive API documentation.

## ğŸ“‹ Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Technologies](#technologies)
- [Installation](#installation)
- [Configuration](#configuration)
- [API Documentation](#api-documentation)
- [API Endpoints](#api-endpoints)
- [Usage Examples](#usage-examples)
- [Project Structure](#project-structure)
- [Development](#development)
- [Testing](#testing)
- [Contributing](#contributing)
- [License](#license)

## âœ¨ Features

- **ğŸ•·ï¸ Web Scraping**: Automated scraping of Glovo restaurant pages using Puppeteer
- **ğŸª Establishment Discovery**: Scrape all restaurants from category pages
- **ğŸ½ï¸ Dish Recommendations**: Intelligent meal planning with budget optimization
- **ğŸ“Š Price Analytics**: Statistical analysis of dish prices across stores
- **ğŸ“– Interactive Documentation**: Complete Swagger API documentation
- **ğŸ—ï¸ MVC Architecture**: Clean, maintainable code structure
- **ğŸ”„ Multiple Algorithms**: Exact, optimized, and greedy recommendation algorithms
- **ğŸ–¼ï¸ Image Processing**: Automatic conversion of dish images to base64
- **ğŸ’¾ Data Persistence**: JSON-based data storage system

## ğŸ—ï¸ Architecture

This application follows the **MVC (Model-View-Controller)** architectural pattern:

```
â”œâ”€â”€ controllers/          # Business logic layer
â”‚   â”œâ”€â”€ scrapeController.js
â”‚   â”œâ”€â”€ storeController.js
â”‚   â”œâ”€â”€ establishmentsController.js
â”‚   â””â”€â”€ recommendationsController.js
â”œâ”€â”€ models/              # Data access layer
â”‚   â””â”€â”€ storeModel.js
â”œâ”€â”€ routers/             # Route definitions
â”‚   â”œâ”€â”€ scrapeRoutes.js
â”‚   â”œâ”€â”€ storeRoutes.js
â”‚   â”œâ”€â”€ establishmentsRoutes.js
â”‚   â””â”€â”€ recommendationsRoutes.js
â”œâ”€â”€ middlewares/         # Express middlewares
â”‚   â”œâ”€â”€ index.js
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ helpers.js
â””â”€â”€ swagger.js           # API documentation
```

## ğŸ› ï¸ Technologies

### Backend
- **Node.js** - Runtime environment
- **Express.js** - Web framework
- **Puppeteer** - Headless browser automation
- **Cheerio** - HTML parsing (backup scraper)

### Documentation & Testing
- **Swagger UI Express** - Interactive API documentation
- **Swagger JSDoc** - OpenAPI specification generation

### Utilities
- **Axios** - HTTP client for image downloads
- **CORS** - Cross-origin resource sharing
- **Dotenv** - Environment variable management
- **Nodemon** - Development auto-restart

## ğŸš€ Installation

### Prerequisites
- Node.js (v16 or higher)
- npm or yarn
- Windows/Linux/MacOS

### Setup Steps

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd dish-scraper-api/backend
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Configure environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your configuration
   ```

4. **Start the development server**
   ```bash
   npm run dev
   ```

5. **Access the application**
   - API Base URL: `http://localhost:3000`
   - API Documentation: `http://localhost:3000/api-docs`

## âš™ï¸ Configuration

Create a `.env` file in the root directory:

```env
# Server Configuration
PORT=3000
NODE_ENV=development

# Glovo Configuration
GLOVO_BASE_URL=https://glovoapp.com
DEFAULT_LOCATION=Casablanca, Morocco

# Scraping Configuration
HEADLESS_BROWSER=false
BROWSER_TIMEOUT=30000
SCROLL_ATTEMPTS=20

# Data Storage
DATA_DIR=./data
SCREENSHOTS_DIR=./assets/screenshots
```

## ğŸ“– API Documentation

The API is fully documented using Swagger/OpenAPI 3.0 specification.

### Access Documentation
- **URL**: `http://localhost:3000/api-docs`
- **Format**: Interactive web interface
- **Features**:
  - Live API testing
  - Request/response examples
  - Schema validation
  - Download OpenAPI spec

## ğŸ”— API Endpoints

### Store Scraping
- `POST /api/scrape` - Scrape a specific Glovo store
- `GET /api/scrape` - Scrape store via query parameters

### Store Management
- `GET /api/stores` - Get all stored stores
- `GET /api/stores/:storeName` - Get specific store details
- `DELETE /api/stores/:storeName` - Delete a store
- `GET /api/stores/:storeName/products` - Get store products
- `DELETE /api/stores` - Clear all stores
- `GET /api/debug/data` - Debug data information

### Establishment Discovery
- `POST /api/establishments` - Scrape establishments from category
- `GET /api/establishments` - Scrape establishments via query

### Recommendations & Analytics
- `POST /api/recommendations` - Get dish recommendations
- `GET /api/recommendations` - Get recommendations via query
- `GET /api/price-stats` - Get price statistics

## ğŸ’¡ Usage Examples

### Scrape a Store
```bash
curl -X POST http://localhost:3000/api/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://glovoapp.com/fr/ma/casablanca/stores/joa",
    "storeName": "JOA"
  }'
```

### Get Recommendations
```bash
curl -X POST http://localhost:3000/api/recommendations \
  -H "Content-Type: application/json" \
  -d '{
    "budget": "100 MAD",
    "numPlates": 3,
    "algorithm": "optimized"
  }'
```

### Scrape Establishments
```bash
curl -X POST http://localhost:3000/api/establishments \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://glovoapp.com/fr/ma/casablanca/categories/pizza"
  }'
```

### Get Price Statistics
```bash
curl http://localhost:3000/api/price-stats
```

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ screenshots/          # Scraping screenshots
â”œâ”€â”€ controllers/              # Business logic
â”‚   â”œâ”€â”€ establishmentsController.js
â”‚   â”œâ”€â”€ recommendationsController.js
â”‚   â”œâ”€â”€ scrapeController.js
â”‚   â””â”€â”€ storeController.js
â”œâ”€â”€ data/                     # JSON data storage
â”‚   â”œâ”€â”€ stores.json
â”‚   â””â”€â”€ establishments_*.json
â”œâ”€â”€ middlewares/              # Express middlewares
â”‚   â”œâ”€â”€ index.js
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ helpers.js
â”œâ”€â”€ models/                   # Data models
â”‚   â””â”€â”€ storeModel.js
â”œâ”€â”€ routers/                  # Route definitions
â”‚   â”œâ”€â”€ establishmentsRoutes.js
â”‚   â”œâ”€â”€ recommendationsRoutes.js
â”‚   â”œâ”€â”€ scrapeRoutes.js
â”‚   â””â”€â”€ storeRoutes.js
â”œâ”€â”€ node_modules/             # Dependencies
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ index.js                  # Application entry point
â”œâ”€â”€ package.json              # Project metadata
â”œâ”€â”€ swagger.js                # API documentation config
â””â”€â”€ README.md                 # This file
```

## ğŸ§ª Testing

### Manual Testing
1. Start the server: `npm run dev`
2. Visit API docs: `http://localhost:3000/api-docs`
3. Use the interactive Swagger UI to test endpoints

### Automated Testing Scripts
```bash
# Test all endpoints
node test-endpoints.js

# Test Swagger documentation
node test-swagger.js
```

### Test Data
The application includes sample data in the `data/` directory:
- `stores.json` - Scraped restaurant data
- Various establishment JSON files

## ğŸ”§ Development

### Available Scripts
```bash
npm start      # Start production server
npm run dev    # Start development server (with nodemon)
```

### Code Style
- Use ES6+ syntax
- Follow MVC pattern
- Add JSDoc comments for Swagger documentation
- Use async/await for asynchronous operations

### Adding New Features
1. Create controller logic in `controllers/`
2. Define routes in `routers/`
3. Add Swagger documentation
4. Update this README

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Commit changes: `git commit -am 'Add feature'`
4. Push to branch: `git push origin feature-name`
5. Submit a pull request

### Guidelines
- Follow existing code style
- Add tests for new features
- Update documentation
- Ensure all tests pass

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## âš ï¸ Disclaimer

This project is for educational purposes only. Web scraping should be done responsibly and in accordance with the target website's terms of service. Always respect robots.txt and implement appropriate rate limiting.

## ğŸ“ Support

For questions or issues:
- Check the API documentation at `/api-docs`
- Review the code comments
- Create an issue in the repository

---

**Built with â¤ï¸ for food lovers and developers**